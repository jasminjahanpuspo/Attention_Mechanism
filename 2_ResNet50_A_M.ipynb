{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP6YvL39xevRdye2tihIKLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasminjahanpuspo/Attention_Mechanism/blob/main/2_ResNet50_A_M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**<font color='yellow'>Model Name: Resnet50_Attention_Mechanism</font>**\n"
      ],
      "metadata": {
        "id": "ErsqFCbL7i55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup Environment"
      ],
      "metadata": {
        "id": "taYt-0Q2jQ7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸš€ Mount Google Drive\n",
        "*   Access datasets stored in your Google Drive.  \n",
        "*   After running this cell, you'll be prompted to authorize access.\n"
      ],
      "metadata": {
        "id": "XpAB5dRKbqKU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQG5VOHy4zdX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ›  Import Required Libraries\n",
        "*   Load all necessary libraries for image processing, data handling, and visualization.\n",
        "*   Load TensorFlow, Keras, and layers for building CNN models."
      ],
      "metadata": {
        "id": "1TYMYAEqbyUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Libraries for Data Handling -------------------- #\n",
        "import numpy as np              # Numerical operations and arrays\n",
        "import pandas as pd             # Data manipulation and analysis\n",
        "import os                       # File and directory operations\n",
        "import glob as gb               # File pattern matching (e.g., get all image paths)\n",
        "\n",
        "# -------------------- Libraries for Image Processing ---------------- #\n",
        "import cv2                      # OpenCV for image reading, processing, and augmentation\n",
        "\n",
        "# -------------------- Libraries for Visualization ------------------ #\n",
        "import matplotlib.pyplot as plt # Plotting graphs and images\n",
        "import seaborn as sns           # Advanced visualizations (heatmaps, pairplots)\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "\n",
        "from PIL import Image\n",
        "import random\n",
        "import math\n"
      ],
      "metadata": {
        "id": "yl9sPyjy4_Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- TensorFlow & Keras -------------------- #\n",
        "import tensorflow as tf                       # Core TensorFlow library\n",
        "from tensorflow import keras                  # High-level API for building neural networks\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# -------------------- Dataset Utilities -------------------- #\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "# Load images from directories into TensorFlow datasets\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "# -------------------- Layers for CNN Models ---------------- #\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, LeakyReLU"
      ],
      "metadata": {
        "id": "4nGEvjWn5AqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prepare Dataset"
      ],
      "metadata": {
        "id": "AiFnZABsjKSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Create TensorFlow Datasets\n",
        "\n",
        "\n",
        "*   Load images from the directories into TensorFlow datasets for training, validation, and testing.  \n",
        "*   You can adjust `image_size` and `batch_size` as needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "nVdbFrrNcQVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the directories for training, testing, and validation\n",
        "train_directory = '/content/drive/MyDrive/sample_dataset/train'\n",
        "test_directory = '/content/drive/MyDrive/sample_dataset/test'\n",
        "valid_directory = '/content/drive/MyDrive/sample_dataset/valid'"
      ],
      "metadata": {
        "id": "qSyUxU6b5BYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224, 224)  # define resolution (299,299) /(224,224)\n",
        "BATCH_SIZE = 128       # varies from dataset to datset prefferable 128/68/32"
      ],
      "metadata": {
        "id": "b-HxLOvn5C-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow datasets for training, testing, and validation\n",
        "#you can customize parameters as per dataset\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    test_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "valid_dataset = image_dataset_from_directory(\n",
        "    valid_directory,\n",
        "    shuffle=True,\n",
        "    labels='inferred',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    color_mode='rgb',\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "lxyfzw5w5GNb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically detect number of classes from dataset\n",
        "class_names = train_dataset.class_names  # works with image_dataset_from_directory\n",
        "n_classes = len(class_names)\n",
        "\n",
        "print(\"Detected Classes:\", class_names)\n",
        "print(\"Number of Classes:\", n_classes)"
      ],
      "metadata": {
        "id": "fFY8fkC45MnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = []\n",
        "for i in range(len(train_dataset.class_names)):\n",
        "    class_counts.append(sum(1 for _, label in train_dataset.unbatch().as_numpy_iterator() if label == i))\n",
        "\n",
        "Dataset = class_counts  # now your class weights will adapt automatically"
      ],
      "metadata": {
        "id": "5uNt6oML5TiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**<font color='red'>ResNet50 + CBAM (Convolutional Block Attention Module)</font>**\n"
      ],
      "metadata": {
        "id": "2S3Uz5oAX_13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.1: Model Training"
      ],
      "metadata": {
        "id": "a4tLGKOajBE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”— CNN with CBAM\n",
        "Add **CBAM (Convolutional Block Attention Module)** on top of a CNN backbone to enhance feature representation with **channel and spatial attention**.\n"
      ],
      "metadata": {
        "id": "qqsw27fpfhbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# CBAM Implementation in Keras\n",
        "# -----------------------------\n",
        "class ChannelAttention(layers.Layer):\n",
        "    def __init__(self, channels, ratio=8):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = layers.GlobalAveragePooling2D()\n",
        "        self.max_pool = layers.GlobalMaxPooling2D()\n",
        "\n",
        "        self.fc1 = layers.Dense(channels // ratio, activation=\"relu\", kernel_initializer='he_normal', use_bias=True)\n",
        "        self.fc2 = layers.Dense(channels, kernel_initializer='he_normal', use_bias=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        avg_out = self.fc2(self.fc1(self.avg_pool(inputs)))\n",
        "        max_out = self.fc2(self.fc1(self.max_pool(inputs)))\n",
        "        out = avg_out + max_out\n",
        "        out = tf.nn.sigmoid(out)\n",
        "        out = tf.reshape(out, [-1, 1, 1, inputs.shape[-1]])\n",
        "        return inputs * out\n",
        "\n",
        "\n",
        "class SpatialAttention(layers.Layer):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv1 = layers.Conv2D(1, kernel_size, padding=\"same\", activation=\"sigmoid\", kernel_initializer='he_normal')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        avg_out = tf.reduce_mean(inputs, axis=3, keepdims=True)\n",
        "        max_out = tf.reduce_max(inputs, axis=3, keepdims=True)\n",
        "        concat = tf.concat([avg_out, max_out], axis=3)\n",
        "        return inputs * self.conv1(concat)\n",
        "\n",
        "\n",
        "class CBAM(layers.Layer):\n",
        "    def __init__(self, channels, ratio=8, kernel_size=7, **kwargs):\n",
        "        super(CBAM, self).__init__(**kwargs)  # Accept kwargs like name\n",
        "        self.channel_attention = ChannelAttention(channels, ratio)\n",
        "        self.spatial_attention = SpatialAttention(kernel_size)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.channel_attention(inputs)\n",
        "        x = self.spatial_attention(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Modified Model with CBAM\n",
        "# -----------------------------\n",
        "def create_model(image_shape=(224,224,3), num_classes=n_classes):\n",
        "    base_model = tf.keras.applications.ResNet50(input_shape=image_shape,\n",
        "                                                   include_top=False,\n",
        "                                                   weights=\"imagenet\")\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[0:291]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = tf.keras.Input(shape=image_shape)\n",
        "    x = tf.keras.applications.resnet50.preprocess_input(inputs)\n",
        "    x = base_model(x, training=False)   # Output: (None, 5, 5, 2048)\n",
        "\n",
        "    # Add CBAM with fixed name\n",
        "    x = CBAM(channels=2048, name=\"cbam\")(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "7Ip6_ebl5Sxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Build and Compile Model"
      ],
      "metadata": {
        "id": "rQyNzA6KW4q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Build and Compile\n",
        "# -----------------------------\n",
        "base_learning_rate = 0.0001\n",
        "model = create_model(image_shape=(224,224,3), num_classes=n_classes)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7lHESKzNW3yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Load / Set Model Weights\n",
        "Load pre-trained weights or initialize custom weights for the CNN + CBAM model.\n"
      ],
      "metadata": {
        "id": "qA6AVPd_fwHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you already have Dataset as a list of samples per class\n",
        "# or you can compute from train_dataset using:\n",
        "# class_counts = [sum(1 for _, label in train_dataset.unbatch().as_numpy_iterator() if label==i) for i in range(n_classes)]\n",
        "\n",
        "total = sum(Dataset)        # total number of images\n",
        "class_weight = {}\n",
        "\n",
        "for i, count in enumerate(Dataset):\n",
        "    class_weight[i] = (1 / count) * (total / len(Dataset))\n",
        "\n",
        "# Print class weights\n",
        "for i, w in class_weight.items():\n",
        "    print(f\"Weight for class {i}: {w:.2f}\")\n"
      ],
      "metadata": {
        "id": "jf-hQ8zw5V8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ’¾ Model Checkpoint & Save\n",
        "Save the best model during training using **checkpoints**, and optionally save the final trained model.\n"
      ],
      "metadata": {
        "id": "BbRZouhZf-sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "## set the path name as *dataset/Lr/optimizer_name/model_name*\n",
        "model_filepath=\"/content/drive/MyDrive/sample_dataset/resnet50_cbam-{epoch:02d}-{val_accuracy:.4f}.keras\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath = model_filepath ,\n",
        "    monitor ='val_accuracy',\n",
        "    mode = 'max' ,\n",
        "    save_best_only =True ,\n",
        "    verbose = 1\n",
        ")"
      ],
      "metadata": {
        "id": "dXWbrCVF5ZGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Train the Model\n",
        "Train the CNN + CBAM model using the training dataset, validate on the validation dataset, and store the training history.\n"
      ],
      "metadata": {
        "id": "0rd3YikzgIzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## change hyperparameter such as epoches\n",
        "history = model.fit(train_dataset , verbose=2 , epochs=5 , class_weight=class_weight ,                         validation_data=valid_dataset ,\n",
        "                    callbacks =[checkpoint])"
      ],
      "metadata": {
        "id": "_8Ic2QEW5cZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Test the Model\n",
        "Evaluate the trained model on the test dataset."
      ],
      "metadata": {
        "id": "FvO1L9xt_hHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset , verbose = 1)"
      ],
      "metadata": {
        "id": "iDopjvLRdc0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.1: Results & Visualizations\n"
      ],
      "metadata": {
        "id": "1xZYuYqJiV23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Actual vs Predicted Classes\n",
        "\n",
        "*  Visualize the modelâ€™s predictions compared to true labels on the test dataset.\n",
        "*   Collect **one example per class** from `test_dataset`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4IHIo3cXhioh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = {}\n",
        "for images, labels in test_dataset.unbatch().take(1000):  # take enough to find all classes\n",
        "    class_idx = labels.numpy()\n",
        "    if class_idx not in examples:\n",
        "        examples[class_idx] = images\n",
        "    if len(examples) == n_classes:\n",
        "        break\n",
        "\n",
        "# Plotting\n",
        "cols = 2  # number of columns\n",
        "rows = math.ceil(n_classes / cols)\n",
        "plt.figure(figsize=(cols * 5, rows * 5))\n",
        "\n",
        "for i, class_idx in enumerate(sorted(examples.keys())):\n",
        "    img = examples[class_idx].numpy().astype(\"uint8\")\n",
        "    img_exp = tf.expand_dims(img, 0)  # expand batch dim\n",
        "    predict = model.predict(img_exp)\n",
        "    predicted = class_names[np.argmax(predict)]\n",
        "    actual = class_names[class_idx]\n",
        "\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    color = 'blue' if predicted == actual else 'red'\n",
        "    plt.title(f\"Pred: {predicted}\\nActual: {actual}\", fontsize=12, fontweight='bold', color=color)\n",
        "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9,\n",
        "                        top=0.9, wspace=0.4, hspace=0.4)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eToRZhnE5kB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Actual vs Predicted Images\n",
        "\n",
        "*   Display sample images from the test set with their **true labels and model predictions** for qualitative evaluation.\n",
        "*   Show **three images per class** for qualitative evaluation.\n"
      ],
      "metadata": {
        "id": "8TeKdDl3pBUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load multiple random images per class\n",
        "def load_images_per_class(folder, num_images_per_class=3):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "\n",
        "    for class_idx, subfolder in enumerate(sorted(os.listdir(folder))):\n",
        "        subfolder_path = os.path.join(folder, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            class_names.append(subfolder)\n",
        "            image_files = os.listdir(subfolder_path)\n",
        "            selected_files = random.sample(image_files, min(num_images_per_class, len(image_files)))\n",
        "            for image_file in selected_files:\n",
        "                img_path = os.path.join(subfolder_path, image_file)\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img = img.resize((64, 64))\n",
        "                images.append(np.array(img)/255.0)\n",
        "                labels.append(class_idx)\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "# Path to your test folder\n",
        "test_folder = '/content/drive/MyDrive/sample_dataset/test'\n",
        "\n",
        "# Load images: change num_images_per_class as needed\n",
        "num_images_per_class = 3\n",
        "images, labels, class_names = load_images_per_class(test_folder, num_images_per_class=num_images_per_class)\n",
        "\n",
        "# Example predicted labels (replace with your model predictions)\n",
        "predicted_labels = labels.copy()  # For demo, assume correct predictions\n",
        "\n",
        "# Automatically calculate subplot grid\n",
        "total_images = len(images)\n",
        "cols = 2  # Original + Predicted\n",
        "rows = total_images  # Each image gets a row\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*4))\n",
        "\n",
        "if rows == 1:  # Special case if only 1 image\n",
        "    axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "for i in range(total_images):\n",
        "    img = images[i]\n",
        "    true_label = class_names[labels[i]]\n",
        "    predicted_label = class_names[predicted_labels[i]]\n",
        "\n",
        "    # Original\n",
        "    axes[i, 0].imshow(img)\n",
        "    axes[i, 0].set_title(f'True: {true_label}', fontsize=12, fontweight='bold')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # Predicted\n",
        "    axes[i, 1].imshow(img)\n",
        "    color = 'blue' if true_label == predicted_label else 'red'\n",
        "    axes[i, 1].set_title(f'Predicted: {predicted_label}', fontsize=12, fontweight='bold', color=color)\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ANPqJlk-6P7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ“ˆ Training Accuracy & Loss\n",
        "Visualize the model's **training and validation accuracy and loss** over epochs to assess learning and overfitting."
      ],
      "metadata": {
        "id": "jesO8T_3iCbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(17, 12))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy', fontweight='bold')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,3.0])\n",
        "plt.title('Training and Validation Loss', fontweight='bold')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xXviS1A95kp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Label Binarization & Classification Metrics\n",
        "Binarize class labels and evaluate model performance using metrics like **accuracy, precision, recall, and F1-score**.\n"
      ],
      "metadata": {
        "id": "CA0dZCDtiNmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Initialize empty lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the validation dataset and make predictions\n",
        "for images, labels in test_dataset:\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels.extend(np.argmax(predictions, axis=1))\n",
        "    true_labels.extend(labels.numpy())\n",
        "\n",
        "# Binarize the true and predicted labels\n",
        "true_labels_bin = label_binarize(true_labels, classes=np.unique(true_labels))\n",
        "predicted_labels_bin = label_binarize(predicted_labels, classes=np.unique(predicted_labels))"
      ],
      "metadata": {
        "id": "shYlIGSe5shp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Classification Report\n",
        "\n",
        "Provides precision, recall, F1-score, and support for each class to summarize model performance.\n"
      ],
      "metadata": {
        "id": "LX422pefmAus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "bUfj7opi5u0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ“ˆ ROC curve\n",
        "*   Plot the **ROC curve** to evaluate model performance.\n",
        "*  **One-vs-Rest** for multiclass and\n",
        " **One-vs-One** for binary classification."
      ],
      "metadata": {
        "id": "PVzx3brFksgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Get true labels and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_dataset:     # or valid_dataset if you want\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(preds)\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "print(\"Shape of predictions:\", y_pred.shape)\n"
      ],
      "metadata": {
        "id": "SBbGO9hY50QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarize labels for multiclass ROC\n",
        "y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))"
      ],
      "metadata": {
        "id": "girhuT6Q52Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n"
      ],
      "metadata": {
        "id": "Ixbg2ZuN56Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Automatically generate colors\n",
        "colors = cm.get_cmap('tab20', n_classes)  # 'tab20' or 'tab10', n_classes colors\n",
        "\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors(i), lw=2,\n",
        "             label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve', fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mVFvnZkv591R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Confusion Matrix\n",
        "Visualize the **confusion matrix** to assess class-wise prediction performance."
      ],
      "metadata": {
        "id": "FhhdtV1PlA27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(5, 3))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix', fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "peYDi_mR5_QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Cohen's Kappa\n",
        "Measure agreement between predicted and true labels beyond chance."
      ],
      "metadata": {
        "id": "ZDeKN5ZYlTxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "print(f'Cohen\\'s Kappa: {kappa:.4f}')"
      ],
      "metadata": {
        "id": "69TpUTr-6CNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Matthews Correlation Coefficient (MCC)\n",
        "Assess overall classification quality considering all confusion matrix terms."
      ],
      "metadata": {
        "id": "9A9DdUEElWW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Assuming true_labels and predicted_labels are multiclass labels\n",
        "mcc_values = [matthews_corrcoef(true_labels == i, predicted_labels == i) for i in np.unique(true_labels)]\n",
        "\n",
        "average_mcc = np.mean(mcc_values)\n",
        "print(f'Average Matthews Correlation Coefficient for Multiclass: {average_mcc:.4f}')"
      ],
      "metadata": {
        "id": "p_Dsl3fq6D3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Right vs Wrong Classifier\n",
        "Visualize and analyze **correctly and incorrectly classified samples** to understand model performance."
      ],
      "metadata": {
        "id": "lT_tF8g4nxTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Calculate total right and wrong predictions\n",
        "total_right = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "total_wrong = sum(1 for true, pred in zip(true_labels, predicted_labels) if true != pred)\n",
        "total_samples = len(true_labels)\n",
        "\n",
        "print(\"Total Right Predictions:\", total_right)\n",
        "print(\"Total Wrong Predictions:\", total_wrong)\n",
        "\n",
        "# Calculate percentages\n",
        "right_percentage = (total_right / total_samples) * 100\n",
        "wrong_percentage = (total_wrong / total_samples) * 100\n",
        "\n",
        "# Data for the bar plot\n",
        "labels = ['Right', 'Wrong']\n",
        "percentages = [right_percentage, wrong_percentage]"
      ],
      "metadata": {
        "id": "zBx-i2nE6K-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(labels, percentages, color=['cyan', 'blue'])\n",
        "\n",
        "# Add percentage labels above the bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.1f}%',\n",
        "             ha='center', fontweight='bold', va='bottom')  # va: vertical alignment\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Right and Wrong Predictions', fontweight='bold')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.ylim(0, 100)  # Set y-axis limit to 100%\n",
        "plt.axhline(0, color='grey', linewidth=0.8, linestyle='--')  # Optional: Add a horizontal line at y=0\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0WR_vi0v6OCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Sensitivity & Specificity\n",
        "Evaluate each class's **sensitivity (recall)** and **specificity** based on true positives and true negatives\n"
      ],
      "metadata": {
        "id": "LAlNmbWKpMWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "# If binary classification\n",
        "if n_classes == 2:\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    print(\"Binary Classification:\")\n",
        "    print(f\"Sensitivity (Recall): {sensitivity:.3f}\")\n",
        "    print(f\"Specificity: {specificity:.3f}\")\n",
        "\n",
        "# If multiclass classification\n",
        "else:\n",
        "    # Sensitivity = TP / (TP + FN)\n",
        "    sensitivity = np.diag(cm) / np.sum(cm, axis=1)\n",
        "    # Specificity = TN / (TN + FP)\n",
        "    specificity = []\n",
        "    for i in range(n_classes):\n",
        "        # For each class, treat it as \"positive\" vs \"rest\"\n",
        "        tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
        "        fp = np.sum(np.delete(cm, i, axis=0)[:, i])\n",
        "        specificity.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
        "    specificity = np.array(specificity)\n",
        "\n",
        "    print(\"Multiclass Classification:\")\n",
        "    for i in range(n_classes):\n",
        "        print(f\"Class {i}: Sensitivity={sensitivity[i]:.3f}, Specificity={specificity[i]:.3f}\")\n",
        "\n",
        "    print(f\"\\nAverage Sensitivity: {np.mean(sensitivity):.3f}\")\n",
        "    print(f\"Average Specificity: {np.mean(specificity):.3f}\")\n"
      ],
      "metadata": {
        "id": "GuMV72ft6S4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot Sensitivity and Specificity ---\n",
        "if n_classes == 2:\n",
        "    metrics = ['Sensitivity', 'Specificity']\n",
        "    values = [sensitivity, specificity]\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.bar(metrics, values, color=['#1f77b4', '#ff7f0e'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"Binary Classification Metrics\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    x = np.arange(n_classes)\n",
        "    width = 0.35  # Bar width\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(x - width/2, sensitivity, width, label='Sensitivity', color='#1f77b4')\n",
        "    plt.bar(x + width/2, specificity, width, label='Specificity', color='#ff7f0e')\n",
        "\n",
        "    plt.xticks(x, [f'Class {i}' for i in range(n_classes)])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"Per-Class Sensitivity and Specificity\", fontsize=12, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "7GtbCbgh6YwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "# Sensitivity (macro average): TP / (TP + FN)\n",
        "sensitivity_per_class = np.diag(cm) / np.sum(cm, axis=1)\n",
        "sensitivity = np.nanmean(sensitivity_per_class)\n",
        "\n",
        "# Specificity (macro average): TN / (TN + FP)\n",
        "specificity_list = []\n",
        "for i in range(n_classes):\n",
        "    tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
        "    fp = np.sum(np.delete(cm, i, axis=0)[:, i])\n",
        "    specificity_list.append(tn / (tn + fp) if (tn + fp) > 0 else np.nan)\n",
        "specificity = np.nanmean(specificity_list)\n",
        "\n",
        "print(f\"Overall Sensitivity (Recall): {sensitivity:.3f}\")\n",
        "print(f\"Overall Specificity: {specificity:.3f}\")\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.bar(['Sensitivity', 'Specificity'], [sensitivity, specificity], color=['skyblue', 'salmon'])\n",
        "plt.title('Overall Sensitivity and Specificity', fontsize=12, fontweight='bold')\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qsU8W7EY6ZUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Jaccard Index & Dice Score\n",
        "Measure overlap between predicted and true classes using **Jaccard Index (IoU)** and **Dice Score**.\n"
      ],
      "metadata": {
        "id": "pNzXqwpxpTqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "jaccard_per_class = []\n",
        "dice_per_class = []\n",
        "\n",
        "for i in range(n_classes):\n",
        "    tp = cm[i, i]\n",
        "    fp = np.sum(cm[:, i]) - tp\n",
        "    fn = np.sum(cm[i, :]) - tp\n",
        "\n",
        "    # Jaccard Index\n",
        "    jaccard = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else np.nan\n",
        "    jaccard_per_class.append(jaccard)\n",
        "\n",
        "    # Dice Score = 2*TP / (2*TP + FP + FN)\n",
        "    dice = 2*tp / (2*tp + fp + fn) if (2*tp + fp + fn) > 0 else np.nan\n",
        "    dice_per_class.append(dice)\n",
        "\n",
        "# Macro-average (overall)\n",
        "jaccard_index = np.nanmean(jaccard_per_class)\n",
        "dice_score = np.nanmean(dice_per_class)\n",
        "\n",
        "print(f\"Overall Jaccard Index: {jaccard_index:.3f}\")\n",
        "print(f\"Overall Dice Score: {dice_score:.3f}\")\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(['Jaccard Index', 'Dice Score'], [jaccard_index, dice_score], color=['lightgreen', 'skyblue'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('Overall Jaccard Index and Dice Score', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "93MdRURn6cgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Model Evaluation Metrics\n",
        "Summarize the model's performance using multiple metrics: **Accuracy**, **Precision**, **Recall (Sensitivity)**, **F1 Score**, **Negative Predictive Value (NPV)**, **AUC-ROC**"
      ],
      "metadata": {
        "id": "1VHqbDJMpfpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Number of classes\n",
        "n_classes = len(np.unique(true_labels))\n",
        "\n",
        "# Accuracy, F1, Precision, Recall (macro average for multiclass)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "\n",
        "# For binary: compute NPV and AUC\n",
        "if n_classes == 2:\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "    auc = roc_auc_score(true_labels, predicted_labels)\n",
        "else:\n",
        "    npv = np.nan  # Not defined for multiclass\n",
        "    auc = np.nan  # Not defined for multiclass\n",
        "\n",
        "# Store metrics in a dictionary\n",
        "metrics = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'NPV': npv,\n",
        "    'AUC-ROC': auc\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['blue', 'orange', 'green', 'red', 'purple', 'cyan'])\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Evaluation Metrics', fontsize=12, fontweight='bold')\n",
        "plt.ylim(0, 1)\n",
        "plt.axhline(y=0.5, color='grey', linestyle='--')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6c_Rl8bF6j0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print metric values\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "KPqCT0kN6ldq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¥ Grad-CAM Visualization\n",
        "Generate **Grad-CAM heatmaps** to visualize the regions of input images that the **CNN + CBAM** model focuses on for its predictions.\n"
      ],
      "metadata": {
        "id": "hVGrB2pvpmew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Grad-CAM Function\n",
        "# -----------------------------\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# Display Heatmap Function\n",
        "# -----------------------------\n",
        "def display_gradcam(img_path, heatmap, alpha=0.4):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    img = image.img_to_array(img)\n",
        "\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = np.expand_dims(heatmap, axis=2)\n",
        "    heatmap = np.repeat(heatmap, 3, axis=2)\n",
        "    heatmap = tf.image.resize(heatmap, (img.shape[0], img.shape[1])).numpy()\n",
        "\n",
        "    superimposed_img = heatmap * alpha + img\n",
        "    superimposed_img = np.uint8(superimposed_img)\n",
        "\n",
        "    plt.imshow(superimposed_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Loop Through Classes\n",
        "# -----------------------------\n",
        "def gradcam_per_class(model, data_dir, last_conv_layer_name=\"cbam\"):\n",
        "\n",
        "    class_folders = sorted(os.listdir(data_dir))\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        # Take first image in folder\n",
        "        img_name = os.listdir(class_path)[0]\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img = image.load_img(img_path, target_size=(224,224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = tf.keras.applications.inception_v3.preprocess_input(img_array)\n",
        "\n",
        "        # Generate Grad-CAM\n",
        "        heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "\n",
        "        # Plot\n",
        "        print(f\"Grad-CAM for class: {class_name}, image: {img_name}\")\n",
        "        display_gradcam(img_path, heatmap)\n",
        "\n",
        "# -----------------------------\n",
        "# Usage\n",
        "# -----------------------------\n",
        "data_dir = \"/content/drive/MyDrive/sample_dataset/test\"  # folder containing subfolders of each class\n",
        "gradcam_per_class(model, data_dir)\n"
      ],
      "metadata": {
        "id": "KNmnGNygLrZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# -----------------------------\n",
        "# Grad-CAM Function\n",
        "# -----------------------------\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# Overlay Heatmap on Image\n",
        "# -----------------------------\n",
        "def overlay_heatmap_on_image(img_path, heatmap, alpha=0.4):\n",
        "    # Load original image\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img = img.resize((224,224))\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Convert heatmap to RGB using jet colormap\n",
        "    # Use matplotlib.colormaps.get_cmap to access the colormap\n",
        "    jet_colormap = matplotlib.colormaps.get_cmap('jet')\n",
        "    heatmap_colored = jet_colormap(heatmap)[..., :3]  # RGB channels only\n",
        "    heatmap_colored = np.uint8(heatmap_colored * 255)\n",
        "    heatmap_image = Image.fromarray(heatmap_colored)\n",
        "\n",
        "    # Resize heatmap to match original image using updated Pillow method\n",
        "    heatmap_image = heatmap_image.resize(img_array.shape[:2][::-1], Image.Resampling.LANCZOS)\n",
        "    heatmap_resized = np.array(heatmap_image)\n",
        "\n",
        "    # Superimpose heatmap\n",
        "    superimposed_img = np.uint8(alpha * heatmap_resized + (1 - alpha) * img_array)\n",
        "\n",
        "    return Image.fromarray(superimposed_img)\n",
        "\n",
        "# -----------------------------\n",
        "# Grad-CAM for Each Class\n",
        "# -----------------------------\n",
        "def gradcam_per_class(model, data_dir):\n",
        "    # Automatically get CBAM layer name\n",
        "    cbam_layer_name = [layer.name for layer in model.layers if \"cbam\" in layer.name][0]\n",
        "    print(\"Using CBAM layer:\", cbam_layer_name)\n",
        "\n",
        "    class_folders = sorted(os.listdir(data_dir))\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        # Pick first image of class\n",
        "        img_name = os.listdir(class_path)[0]\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img = image.load_img(img_path, target_size=(224,224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = tf.keras.applications.inception_v3.preprocess_input(img_array)\n",
        "\n",
        "        # Compute Grad-CAM heatmap\n",
        "        heatmap = make_gradcam_heatmap(img_array, model, cbam_layer_name)\n",
        "\n",
        "        # Overlay heatmap\n",
        "        superimposed_img = overlay_heatmap_on_image(img_path, heatmap)\n",
        "\n",
        "        # Display\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.title(f\"Class: {class_name}, Image: {img_name}\")\n",
        "        plt.imshow(superimposed_img)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Usage\n",
        "# -----------------------------\n",
        "data_dir = \"/content/drive/MyDrive/sample_dataset/test\"  # folder with subfolders for each class\n",
        "gradcam_per_class(model, data_dir)"
      ],
      "metadata": {
        "id": "EtzRl4W7Otp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ PR-AUC (Precision-Recall AUC)\n",
        "Evaluate model performance using the **area under the Precision-Recall curve**, especially useful for imbalanced datasets."
      ],
      "metadata": {
        "id": "Af0DR9JV7cdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Get true labels and predicted probabilities\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred_probs = model.predict(test_dataset)\n",
        "\n",
        "# Handle binary or multiclass\n",
        "if y_pred_probs.shape[1] == 2:  # binary\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred_probs[:, 1])\n",
        "    pr_auc = auc(recall, precision)\n",
        "else:  # multiclass\n",
        "    pr_auc = {}\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "    y_true_bin = label_binarize(y_true, classes=range(y_pred_probs.shape[1]))\n",
        "    for i in range(y_pred_probs.shape[1]):\n",
        "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_probs[:, i])\n",
        "        pr_auc[i] = auc(recall, precision)\n",
        "\n",
        "print(\"PR-AUC:\", pr_auc)\n"
      ],
      "metadata": {
        "id": "NWBl9cUJ7buq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Log Loss / Cross-Entropy Loss\n",
        "Evaluate prediction confidence using **log loss** (cross-entropy) between true and predicted probabilities."
      ],
      "metadata": {
        "id": "Hc2bZv8B8VDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred_probs = model.predict(test_dataset)\n",
        "\n",
        "loss = log_loss(y_true, y_pred_probs)\n",
        "print(\"Log Loss / Cross-Entropy Loss:\", loss)\n"
      ],
      "metadata": {
        "id": "x4MzdGOK8VvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Top-k Accuracy\n",
        "Measure if the **true label** is among the model's **top k predicted classes** in multiclass classification."
      ],
      "metadata": {
        "id": "v0k9a8LD8YW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k accuracy automatically for multiclass\n",
        "k = 3  # you can change k\n",
        "top_k_acc = tf.keras.metrics.TopKCategoricalAccuracy(k=k)\n",
        "\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred_probs = model.predict(test_dataset)\n",
        "\n",
        "# Convert binary labels to categorical if needed\n",
        "if y_pred_probs.shape[1] == 2 and len(np.unique(y_true)) == 2:\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    y_true_cat = to_categorical(y_true, num_classes=2)\n",
        "else:\n",
        "    y_true_cat = tf.keras.utils.to_categorical(y_true, num_classes=y_pred_probs.shape[1])\n",
        "\n",
        "top_k_acc.update_state(y_true_cat, y_pred_probs)\n",
        "print(f\"Top-{k} Accuracy:\", top_k_acc.result().numpy())"
      ],
      "metadata": {
        "id": "E0kDKIfy8Y-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ G-Mean (Geometric Mean of Sensitivity & Specificity)\n",
        "Compute the **G-Mean** to evaluate balanced classification performance."
      ],
      "metadata": {
        "id": "m8F6HmNl8bPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred = np.argmax(model.predict(test_dataset), axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "if cm.shape[0] == 2:  # binary\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    g_mean = math.sqrt(sensitivity * specificity)\n",
        "else:  # multiclass: compute G-mean per class and average\n",
        "    sensitivity_list = []\n",
        "    specificity_list = []\n",
        "    for i in range(cm.shape[0]):\n",
        "        tp = cm[i, i]\n",
        "        fn = cm[i, :].sum() - tp\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        tn = cm.sum() - (tp + fn + fp)\n",
        "        sensitivity_list.append(tp / (tp + fn) if (tp+fn)>0 else 0)\n",
        "        specificity_list.append(tn / (tn + fp) if (tn+fp)>0 else 0)\n",
        "    g_mean = np.mean(np.sqrt(np.array(sensitivity_list) * np.array(specificity_list)))\n",
        "\n",
        "print(\"G-Mean:\", g_mean)"
      ],
      "metadata": {
        "id": "9YnFJJv18c7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**<font color='red'>ResNet50  + SE (Squeeze-and-Excitation)</font>**\n"
      ],
      "metadata": {
        "id": "pYukM1_xTKfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.2: Model Training"
      ],
      "metadata": {
        "id": "FUfzghycpBMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”— CNN with SE\n",
        "Add **SE (Squeeze-and-Excitation)** blocks to help the model focus more on the **important channels** in the image."
      ],
      "metadata": {
        "id": "ej1ogNPWFG9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# -----------------------------\n",
        "# SE Block\n",
        "# -----------------------------\n",
        "class SEBlock(layers.Layer):\n",
        "    def __init__(self, channels, ratio=8, **kwargs):  # add **kwargs\n",
        "        super(SEBlock, self).__init__(**kwargs)       # pass kwargs to parent\n",
        "        self.global_avg_pool = layers.GlobalAveragePooling2D()\n",
        "        self.fc1 = layers.Dense(channels // ratio, activation='relu', kernel_initializer='he_normal')\n",
        "        self.fc2 = layers.Dense(channels, activation='sigmoid', kernel_initializer='he_normal')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        se = self.global_avg_pool(inputs)\n",
        "        se = self.fc1(se)\n",
        "        se = self.fc2(se)\n",
        "        se = tf.reshape(se, [-1, 1, 1, inputs.shape[-1]])\n",
        "        return inputs * se\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model with SE\n",
        "# -----------------------------\n",
        "def create_model(image_shape=(224,224,3), num_classes=n_classes):\n",
        "    base_model = tf.keras.applications.ResNet50(input_shape=image_shape,\n",
        "                                                   include_top=False,\n",
        "                                                   weights=\"imagenet\")\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[0:291]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = tf.keras.Input(shape=image_shape)\n",
        "    x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
        "    x = base_model(x, training=False)   # (None, 5, 5, 2048)\n",
        "\n",
        "    # ðŸ”¥ Insert SE here\n",
        "    x = SEBlock(channels=2048, name=\"se_block\")(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs)\n"
      ],
      "metadata": {
        "id": "hA9947bhTRZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Build and Compile Model"
      ],
      "metadata": {
        "id": "6cQU3elToPC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Build and Compile\n",
        "# -----------------------------\n",
        "base_learning_rate = 0.0001\n",
        "model = create_resnet_se(image_shape=(224,224,3), num_classes=n_classes)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Nadam(learning_rate=base_learning_rate),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sA2YX3SdoPC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Load / Set Model Weights\n",
        "Load pre-trained weights or initialize custom weights for the CNN + CBAM model.\n"
      ],
      "metadata": {
        "id": "7nFIdFm3oPC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you already have Dataset as a list of samples per class\n",
        "# or you can compute from train_dataset using:\n",
        "# class_counts = [sum(1 for _, label in train_dataset.unbatch().as_numpy_iterator() if label==i) for i in range(n_classes)]\n",
        "\n",
        "total = sum(Dataset)        # total number of images\n",
        "class_weight = {}\n",
        "\n",
        "for i, count in enumerate(Dataset):\n",
        "    class_weight[i] = (1 / count) * (total / len(Dataset))\n",
        "\n",
        "# Print class weights\n",
        "for i, w in class_weight.items():\n",
        "    print(f\"Weight for class {i}: {w:.2f}\")\n"
      ],
      "metadata": {
        "id": "OGylBw4KoPC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Model Checkpoint & Save\n",
        "Save the best model during training using **checkpoints**, and optionally save the final trained model.\n"
      ],
      "metadata": {
        "id": "iJAiNlOqoPC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "## set the path name as *dataset/Lr/optimizer_name/model_name*\n",
        "model_filepath=\"/content/drive/MyDrive/sample_dataset/resnet_se-{epoch:02d}-{val_accuracy:.4f}.keras\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath = model_filepath ,\n",
        "    monitor ='val_accuracy',\n",
        "    mode = 'max' ,\n",
        "    save_best_only =True ,\n",
        "    verbose = 1\n",
        ")\n"
      ],
      "metadata": {
        "id": "rhMTGl8eoPC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Train the Model\n",
        "Train the CNN + CBAM model using the training dataset, validate on the validation dataset, and store the training history.\n"
      ],
      "metadata": {
        "id": "RyXhOZRAoPC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## change hyperparameter such as epoches\n",
        "history = model.fit(train_dataset , verbose=2 , epochs=5 , class_weight=class_weight ,                         validation_data=valid_dataset ,\n",
        "                    callbacks =[checkpoint])"
      ],
      "metadata": {
        "id": "uGAtSEEzoPC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Test the Model\n",
        "Evaluate the trained model on the test dataset."
      ],
      "metadata": {
        "id": "nIH2nf2kXC6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset , verbose = 1)"
      ],
      "metadata": {
        "id": "l7m3_9bOXC6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.2: Results & Visualizations\n"
      ],
      "metadata": {
        "id": "fQy88_O5oPC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Actual vs Predicted Classes\n",
        "\n",
        "*  Visualize the modelâ€™s predictions compared to true labels on the test dataset.\n",
        "*   Collect **one example per class** from `test_dataset`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5J34Kx2doPDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = {}\n",
        "for images, labels in test_dataset.unbatch().take(1000):  # take enough to find all classes\n",
        "    class_idx = labels.numpy()\n",
        "    if class_idx not in examples:\n",
        "        examples[class_idx] = images\n",
        "    if len(examples) == n_classes:\n",
        "        break\n",
        "\n",
        "# Plotting\n",
        "cols = 2  # number of columns\n",
        "rows = math.ceil(n_classes / cols)\n",
        "plt.figure(figsize=(cols * 5, rows * 5))\n",
        "\n",
        "for i, class_idx in enumerate(sorted(examples.keys())):\n",
        "    img = examples[class_idx].numpy().astype(\"uint8\")\n",
        "    img_exp = tf.expand_dims(img, 0)  # expand batch dim\n",
        "    predict = model.predict(img_exp)\n",
        "    predicted = class_names[np.argmax(predict)]\n",
        "    actual = class_names[class_idx]\n",
        "\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    color = 'blue' if predicted == actual else 'red'\n",
        "    plt.title(f\"Pred: {predicted}\\nActual: {actual}\", fontsize=12, fontweight='bold', color=color)\n",
        "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9,\n",
        "                        top=0.9, wspace=0.4, hspace=0.4)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ujVTbUejoPDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Actual vs Predicted Images\n",
        "\n",
        "*   Display sample images from the test set with their **true labels and model predictions** for qualitative evaluation.\n",
        "*   Show **three images per class** for qualitative evaluation.\n"
      ],
      "metadata": {
        "id": "nJZMihVzoPDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load multiple random images per class\n",
        "def load_images_per_class(folder, num_images_per_class=3):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "\n",
        "    for class_idx, subfolder in enumerate(sorted(os.listdir(folder))):\n",
        "        subfolder_path = os.path.join(folder, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            class_names.append(subfolder)\n",
        "            image_files = os.listdir(subfolder_path)\n",
        "            selected_files = random.sample(image_files, min(num_images_per_class, len(image_files)))\n",
        "            for image_file in selected_files:\n",
        "                img_path = os.path.join(subfolder_path, image_file)\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img = img.resize((64, 64))\n",
        "                images.append(np.array(img)/255.0)\n",
        "                labels.append(class_idx)\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "# Path to your test folder\n",
        "test_folder = '/content/drive/MyDrive/sample_dataset/test'\n",
        "\n",
        "# Load images: change num_images_per_class as needed\n",
        "num_images_per_class = 3\n",
        "images, labels, class_names = load_images_per_class(test_folder, num_images_per_class=num_images_per_class)\n",
        "\n",
        "# Example predicted labels (replace with your model predictions)\n",
        "predicted_labels = labels.copy()  # For demo, assume correct predictions\n",
        "\n",
        "# Automatically calculate subplot grid\n",
        "total_images = len(images)\n",
        "cols = 2  # Original + Predicted\n",
        "rows = total_images  # Each image gets a row\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*4))\n",
        "\n",
        "if rows == 1:  # Special case if only 1 image\n",
        "    axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "for i in range(total_images):\n",
        "    img = images[i]\n",
        "    true_label = class_names[labels[i]]\n",
        "    predicted_label = class_names[predicted_labels[i]]\n",
        "\n",
        "    # Original\n",
        "    axes[i, 0].imshow(img)\n",
        "    axes[i, 0].set_title(f'True: {true_label}', fontsize=12, fontweight='bold')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # Predicted\n",
        "    axes[i, 1].imshow(img)\n",
        "    color = 'blue' if true_label == predicted_label else 'red'\n",
        "    axes[i, 1].set_title(f'Predicted: {predicted_label}', fontsize=12, fontweight='bold', color=color)\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lEwdznQjoPDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ“ˆ Training Accuracy & Loss\n",
        "Visualize the model's **training and validation accuracy and loss** over epochs to assess learning and overfitting."
      ],
      "metadata": {
        "id": "Qt6-7-z9oPDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(17, 12))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy', fontweight='bold')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,3.0])\n",
        "plt.title('Training and Validation Loss', fontweight='bold')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SPQTpPhQoPDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Label Binarization & Classification Metrics\n",
        "Binarize class labels and evaluate model performance using metrics like **accuracy, precision, recall, and F1-score**.\n"
      ],
      "metadata": {
        "id": "tsOE8s-_oPDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Initialize empty lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the validation dataset and make predictions\n",
        "for images, labels in test_dataset:\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels.extend(np.argmax(predictions, axis=1))\n",
        "    true_labels.extend(labels.numpy())\n",
        "\n",
        "# Binarize the true and predicted labels\n",
        "true_labels_bin = label_binarize(true_labels, classes=np.unique(true_labels))\n",
        "predicted_labels_bin = label_binarize(predicted_labels, classes=np.unique(predicted_labels))"
      ],
      "metadata": {
        "id": "qfu2QV9zoPDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Classification Report\n",
        "Provides precision, recall, F1-score, and support for each class to summarize model performance."
      ],
      "metadata": {
        "id": "pRn7o0iXm81P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "6rkEIHsUoPDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ“ˆ ROC curve\n",
        "*   Plot the **ROC curve** to evaluate model performance.\n",
        "*  **One-vs-Rest** for multiclass and\n",
        " **One-vs-One** for binary classification."
      ],
      "metadata": {
        "id": "CPtPK2-poPDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Get true labels and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_dataset:     # or valid_dataset if you want\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(preds)\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "print(\"Shape of predictions:\", y_pred.shape)\n"
      ],
      "metadata": {
        "id": "YkKcuFeloPDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarize labels for multiclass ROC\n",
        "y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))"
      ],
      "metadata": {
        "id": "WyIiop8woPDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n"
      ],
      "metadata": {
        "id": "iRVgE1g_oPDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Automatically generate colors\n",
        "colors = cm.get_cmap('tab20', n_classes)  # 'tab20' or 'tab10', n_classes colors\n",
        "\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors(i), lw=2,\n",
        "             label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve', fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nBxYKz1UoPDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Confusion Matrix\n",
        "Visualize the **confusion matrix** to assess class-wise prediction performance."
      ],
      "metadata": {
        "id": "Cw-9MbSQoPDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(5, 3))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix', fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EoGalTjToPDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Cohen's Kappa\n",
        "Measure agreement between predicted and true labels beyond chance."
      ],
      "metadata": {
        "id": "7UD6f580oPDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "print(f'Cohen\\'s Kappa: {kappa:.4f}')"
      ],
      "metadata": {
        "id": "d0SLG7tkoPDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Matthews Correlation Coefficient (MCC)\n",
        "Assess overall classification quality considering all confusion matrix terms."
      ],
      "metadata": {
        "id": "gOmHH4i1oPDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Assuming true_labels and predicted_labels are multiclass labels\n",
        "mcc_values = [matthews_corrcoef(true_labels == i, predicted_labels == i) for i in np.unique(true_labels)]\n",
        "\n",
        "average_mcc = np.mean(mcc_values)\n",
        "print(f'Average Matthews Correlation Coefficient for Multiclass: {average_mcc:.4f}')"
      ],
      "metadata": {
        "id": "MprWf4RooPDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Right vs Wrong Classifier\n",
        "Visualize and analyze **correctly and incorrectly classified samples** to understand model performance."
      ],
      "metadata": {
        "id": "8msEmO-moPDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total right and wrong predictions\n",
        "total_right = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "total_wrong = sum(1 for true, pred in zip(true_labels, predicted_labels) if true != pred)\n",
        "total_samples = len(true_labels)\n",
        "\n",
        "# Calculate wrong prediction percentage\n",
        "wrong_prediction_percentage = (total_wrong / total_samples) * 100\n",
        "\n",
        "print(\"Total Right Predictions:\", total_right)\n",
        "print(\"Total Wrong Predictions:\", total_wrong)\n",
        "print(\"Wrong Prediction Percentage: {:.2f}%\".format(wrong_prediction_percentage))\n"
      ],
      "metadata": {
        "id": "1DyWBF1uoPDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for plotting\n",
        "categories = ['Right Predictions', 'Wrong Predictions']\n",
        "values = [total_right, total_wrong]\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(categories, values, color=['green', 'red'])\n",
        "plt.ylabel('Count')\n",
        "plt.title('Right and Wrong Predictions', fontweight='bold')\n",
        "\n",
        "# Show counts on top of the bars\n",
        "for i, v in enumerate(values):\n",
        "    plt.text(i, v + 0.1, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v5beDp7PoPDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Sensitivity & Specificity\n",
        "Evaluate each class's **sensitivity (recall)** and **specificity** based on true positives and true negatives\n"
      ],
      "metadata": {
        "id": "1SD4WPRRoPDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "# If binary classification\n",
        "if n_classes == 2:\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    print(\"Binary Classification:\")\n",
        "    print(f\"Sensitivity (Recall): {sensitivity:.3f}\")\n",
        "    print(f\"Specificity: {specificity:.3f}\")\n",
        "\n",
        "# If multiclass classification\n",
        "else:\n",
        "    # Sensitivity = TP / (TP + FN)\n",
        "    sensitivity = np.diag(cm) / np.sum(cm, axis=1)\n",
        "    # Specificity = TN / (TN + FP)\n",
        "    specificity = []\n",
        "    for i in range(n_classes):\n",
        "        # For each class, treat it as \"positive\" vs \"rest\"\n",
        "        tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
        "        fp = np.sum(np.delete(cm, i, axis=0)[:, i])\n",
        "        specificity.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
        "    specificity = np.array(specificity)\n",
        "\n",
        "    print(\"Multiclass Classification:\")\n",
        "    for i in range(n_classes):\n",
        "        print(f\"Class {i}: Sensitivity={sensitivity[i]:.3f}, Specificity={specificity[i]:.3f}\")\n",
        "\n",
        "    print(f\"\\nAverage Sensitivity: {np.mean(sensitivity):.3f}\")\n",
        "    print(f\"Average Specificity: {np.mean(specificity):.3f}\")\n"
      ],
      "metadata": {
        "id": "Ejk70H6poPDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot Sensitivity and Specificity ---\n",
        "if n_classes == 2:\n",
        "    metrics = ['Sensitivity', 'Specificity']\n",
        "    values = [sensitivity, specificity]\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.bar(metrics, values, color=['#1f77b4', '#ff7f0e'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"Binary Classification Metrics\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    x = np.arange(n_classes)\n",
        "    width = 0.35  # Bar width\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(x - width/2, sensitivity, width, label='Sensitivity', color='#1f77b4')\n",
        "    plt.bar(x + width/2, specificity, width, label='Specificity', color='#ff7f0e')\n",
        "\n",
        "    plt.xticks(x, [f'Class {i}' for i in range(n_classes)])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"Per-Class Sensitivity and Specificity\", fontsize=12, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "31akDBROoPDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "# Sensitivity (macro average): TP / (TP + FN)\n",
        "sensitivity_per_class = np.diag(cm) / np.sum(cm, axis=1)\n",
        "sensitivity = np.nanmean(sensitivity_per_class)\n",
        "\n",
        "# Specificity (macro average): TN / (TN + FP)\n",
        "specificity_list = []\n",
        "for i in range(n_classes):\n",
        "    tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
        "    fp = np.sum(np.delete(cm, i, axis=0)[:, i])\n",
        "    specificity_list.append(tn / (tn + fp) if (tn + fp) > 0 else np.nan)\n",
        "specificity = np.nanmean(specificity_list)\n",
        "\n",
        "print(f\"Overall Sensitivity (Recall): {sensitivity:.3f}\")\n",
        "print(f\"Overall Specificity: {specificity:.3f}\")\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.bar(['Sensitivity', 'Specificity'], [sensitivity, specificity], color=['skyblue', 'salmon'])\n",
        "plt.title('Overall Sensitivity and Specificity', fontsize=12, fontweight='bold')\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p47rs6CxoPDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Jaccard Index & Dice Score\n",
        "Measure overlap between predicted and true classes using **Jaccard Index (IoU)** and **Dice Score**.\n"
      ],
      "metadata": {
        "id": "Ny1J0KOMoPDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "jaccard_per_class = []\n",
        "dice_per_class = []\n",
        "\n",
        "for i in range(n_classes):\n",
        "    tp = cm[i, i]\n",
        "    fp = np.sum(cm[:, i]) - tp\n",
        "    fn = np.sum(cm[i, :]) - tp\n",
        "\n",
        "    # Jaccard Index\n",
        "    jaccard = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else np.nan\n",
        "    jaccard_per_class.append(jaccard)\n",
        "\n",
        "    # Dice Score = 2*TP / (2*TP + FP + FN)\n",
        "    dice = 2*tp / (2*tp + fp + fn) if (2*tp + fp + fn) > 0 else np.nan\n",
        "    dice_per_class.append(dice)\n",
        "\n",
        "# Macro-average (overall)\n",
        "jaccard_index = np.nanmean(jaccard_per_class)\n",
        "dice_score = np.nanmean(dice_per_class)\n",
        "\n",
        "print(f\"Overall Jaccard Index: {jaccard_index:.3f}\")\n",
        "print(f\"Overall Dice Score: {dice_score:.3f}\")\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(['Jaccard Index', 'Dice Score'], [jaccard_index, dice_score], color=['lightgreen', 'skyblue'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('Overall Jaccard Index and Dice Score', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PH2oktkkoPDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Model Evaluation Metrics\n",
        "Summarize the model's performance using multiple metrics: **Accuracy**, **Precision**, **Recall (Sensitivity)**, **F1 Score**, **Negative Predictive Value (NPV)**, **AUC-ROC**"
      ],
      "metadata": {
        "id": "xtSkeIHboPDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Number of classes\n",
        "n_classes = len(np.unique(true_labels))\n",
        "\n",
        "# Accuracy, F1, Precision, Recall (macro average for multiclass)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "\n",
        "# For binary: compute NPV and AUC\n",
        "if n_classes == 2:\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "    auc = roc_auc_score(true_labels, predicted_labels)\n",
        "else:\n",
        "    npv = np.nan  # Not defined for multiclass\n",
        "    auc = np.nan  # Not defined for multiclass\n",
        "\n",
        "# Store metrics in a dictionary\n",
        "metrics = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'NPV': npv,\n",
        "    'AUC-ROC': auc\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['blue', 'orange', 'green', 'red', 'purple', 'cyan'])\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Evaluation Metrics', fontsize=12, fontweight='bold')\n",
        "plt.ylim(0, 1)\n",
        "plt.axhline(y=0.5, color='grey', linestyle='--')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-bQtEbzFoPDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print metric values\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "LoL7vrQ0oPDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¥ Grad-CAM Visualization\n",
        "Generate **Grad-CAM heatmaps** to visualize the regions of input images that the **CNN + CBAM** model focuses on for its predictions.\n"
      ],
      "metadata": {
        "id": "EF20MZqSoPDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Grad-CAM Function\n",
        "# -----------------------------\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# Display Heatmap Function\n",
        "# -----------------------------\n",
        "def display_gradcam(img_path, heatmap, alpha=0.4):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    img = image.img_to_array(img)\n",
        "\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = np.expand_dims(heatmap, axis=2)\n",
        "    heatmap = np.repeat(heatmap, 3, axis=2)\n",
        "    heatmap = tf.image.resize(heatmap, (img.shape[0], img.shape[1])).numpy()\n",
        "\n",
        "    superimposed_img = heatmap * alpha + img\n",
        "    superimposed_img = np.uint8(superimposed_img)\n",
        "\n",
        "    plt.imshow(superimposed_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Loop Through Classes\n",
        "# -----------------------------\n",
        "def gradcam_per_class(model, data_dir, last_conv_layer_name=\"cbam\"):\n",
        "\n",
        "    class_folders = sorted(os.listdir(data_dir))\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        # Take first image in folder\n",
        "        img_name = os.listdir(class_path)[0]\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img = image.load_img(img_path, target_size=(224,224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = tf.keras.applications.inception_v3.preprocess_input(img_array)\n",
        "\n",
        "        # Generate Grad-CAM\n",
        "        heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "\n",
        "        # Plot\n",
        "        print(f\"Grad-CAM for class: {class_name}, image: {img_name}\")\n",
        "        display_gradcam(img_path, heatmap)\n",
        "\n",
        "# -----------------------------\n",
        "# Usage\n",
        "# -----------------------------\n",
        "data_dir = \"/content/drive/MyDrive/sample_dataset/test\"  # folder containing subfolders of each class\n",
        "gradcam_per_class(model, data_dir)\n"
      ],
      "metadata": {
        "id": "WMd_3Gu7oPDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# -----------------------------\n",
        "# Grad-CAM Function\n",
        "# -----------------------------\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# Overlay Heatmap on Image\n",
        "# -----------------------------\n",
        "def overlay_heatmap_on_image(img_path, heatmap, alpha=0.4):\n",
        "    # Load original image\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img = img.resize((224,224))\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Convert heatmap to RGB using jet colormap\n",
        "    # Use matplotlib.colormaps.get_cmap to access the colormap\n",
        "    jet_colormap = matplotlib.colormaps.get_cmap('jet')\n",
        "    heatmap_colored = jet_colormap(heatmap)[..., :3]  # RGB channels only\n",
        "    heatmap_colored = np.uint8(heatmap_colored * 255)\n",
        "    heatmap_image = Image.fromarray(heatmap_colored)\n",
        "\n",
        "    # Resize heatmap to match original image using updated Pillow method\n",
        "    heatmap_image = heatmap_image.resize(img_array.shape[:2][::-1], Image.Resampling.LANCZOS)\n",
        "    heatmap_resized = np.array(heatmap_image)\n",
        "\n",
        "    # Superimpose heatmap\n",
        "    superimposed_img = np.uint8(alpha * heatmap_resized + (1 - alpha) * img_array)\n",
        "\n",
        "    return Image.fromarray(superimposed_img)\n",
        "\n",
        "# -----------------------------\n",
        "# Grad-CAM for Each Class\n",
        "# -----------------------------\n",
        "def gradcam_per_class(model, data_dir):\n",
        "    # Automatically get CBAM layer name\n",
        "    cbam_layer_name = [layer.name for layer in model.layers if \"cbam\" in layer.name][0]\n",
        "    print(\"Using CBAM layer:\", cbam_layer_name)\n",
        "\n",
        "    class_folders = sorted(os.listdir(data_dir))\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        # Pick first image of class\n",
        "        img_name = os.listdir(class_path)[0]\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img = image.load_img(img_path, target_size=(224,224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = tf.keras.applications.inception_v3.preprocess_input(img_array)\n",
        "\n",
        "        # Compute Grad-CAM heatmap\n",
        "        heatmap = make_gradcam_heatmap(img_array, model, cbam_layer_name)\n",
        "\n",
        "        # Overlay heatmap\n",
        "        superimposed_img = overlay_heatmap_on_image(img_path, heatmap)\n",
        "\n",
        "        # Display\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.title(f\"Class: {class_name}, Image: {img_name}\")\n",
        "        plt.imshow(superimposed_img)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Usage\n",
        "# -----------------------------\n",
        "data_dir = \"/content/drive/MyDrive/sample_dataset/test\"  # folder with subfolders for each class\n",
        "gradcam_per_class(model, data_dir)"
      ],
      "metadata": {
        "id": "WE8zldbAoPDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ PR-AUC (Precision-Recall AUC)\n",
        "Evaluate model performance using the **area under the Precision-Recall curve**, especially useful for imbalanced datasets."
      ],
      "metadata": {
        "id": "7gjriez9oPDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Get true labels and predicted probabilities\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred_probs = model.predict(test_dataset)\n",
        "\n",
        "# Handle binary or multiclass\n",
        "if y_pred_probs.shape[1] == 2:  # binary\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred_probs[:, 1])\n",
        "    pr_auc = auc(recall, precision)\n",
        "else:  # multiclass\n",
        "    pr_auc = {}\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "    y_true_bin = label_binarize(y_true, classes=range(y_pred_probs.shape[1]))\n",
        "    for i in range(y_pred_probs.shape[1]):\n",
        "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_probs[:, i])\n",
        "        pr_auc[i] = auc(recall, precision)\n",
        "\n",
        "print(\"PR-AUC:\", pr_auc)\n"
      ],
      "metadata": {
        "id": "XBwGalESoPDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Log Loss / Cross-Entropy Loss\n",
        "Evaluate prediction confidence using **log loss** (cross-entropy) between true and predicted probabilities."
      ],
      "metadata": {
        "id": "bo4RMPZ4oPDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred_probs = model.predict(test_dataset)\n",
        "\n",
        "loss = log_loss(y_true, y_pred_probs)\n",
        "print(\"Log Loss / Cross-Entropy Loss:\", loss)\n"
      ],
      "metadata": {
        "id": "yZU95CT1oPDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Top-k Accuracy\n",
        "Measure if the **true label** is among the model's **top k predicted classes** in multiclass classification."
      ],
      "metadata": {
        "id": "Dm-sqYFWoPDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k accuracy automatically for multiclass\n",
        "k = 3  # you can change k\n",
        "top_k_acc = tf.keras.metrics.TopKCategoricalAccuracy(k=k)\n",
        "\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred_probs = model.predict(test_dataset)\n",
        "\n",
        "# Convert binary labels to categorical if needed\n",
        "if y_pred_probs.shape[1] == 2 and len(np.unique(y_true)) == 2:\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    y_true_cat = to_categorical(y_true, num_classes=2)\n",
        "else:\n",
        "    y_true_cat = tf.keras.utils.to_categorical(y_true, num_classes=y_pred_probs.shape[1])\n",
        "\n",
        "top_k_acc.update_state(y_true_cat, y_pred_probs)\n",
        "print(f\"Top-{k} Accuracy:\", top_k_acc.result().numpy())"
      ],
      "metadata": {
        "id": "6HMZHJ-hoPDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ G-Mean (Geometric Mean of Sensitivity & Specificity)\n",
        "Compute the **G-Mean** to evaluate balanced classification performance."
      ],
      "metadata": {
        "id": "xsr3Yz5woPDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred = np.argmax(model.predict(test_dataset), axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "if cm.shape[0] == 2:  # binary\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    g_mean = math.sqrt(sensitivity * specificity)\n",
        "else:  # multiclass: compute G-mean per class and average\n",
        "    sensitivity_list = []\n",
        "    specificity_list = []\n",
        "    for i in range(cm.shape[0]):\n",
        "        tp = cm[i, i]\n",
        "        fn = cm[i, :].sum() - tp\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        tn = cm.sum() - (tp + fn + fp)\n",
        "        sensitivity_list.append(tp / (tp + fn) if (tp+fn)>0 else 0)\n",
        "        specificity_list.append(tn / (tn + fp) if (tn+fp)>0 else 0)\n",
        "    g_mean = np.mean(np.sqrt(np.array(sensitivity_list) * np.array(specificity_list)))\n",
        "\n",
        "print(\"G-Mean:\", g_mean)"
      ],
      "metadata": {
        "id": "TWT6rZ81oPDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fU6naSj3noUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**<font color='red'>ResNet50 + ECA (Efficient Channel Attention)</font>**\n"
      ],
      "metadata": {
        "id": "2sQGWEDyTVcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3.3: Model Training"
      ],
      "metadata": {
        "id": "4vtaQkATpYVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”— CNN with ECA\n",
        "Add **ECA (Efficient Channel Attention)** modules to let the model **learn which channels are most useful** without adding extra layers."
      ],
      "metadata": {
        "id": "ciwvE8vSGY3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# ECA Block\n",
        "# -----------------------------\n",
        "class ECABlock(layers.Layer):\n",
        "    def __init__(self, channels, gamma=2, b=1, **kwargs):\n",
        "        super(ECABlock, self).__init__(**kwargs)\n",
        "        # Determine adaptive kernel size\n",
        "        t = int(abs((tf.math.log(tf.cast(channels, tf.float32)) / tf.math.log(2.0)) + b) / gamma)\n",
        "        k = t if t % 2 else t + 1\n",
        "        self.avg_pool = layers.GlobalAveragePooling2D()\n",
        "        self.conv1d = layers.Conv1D(1, kernel_size=k, padding=\"same\", use_bias=False)\n",
        "        self.sigmoid = layers.Activation(\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch, H, W, C)\n",
        "        y = self.avg_pool(inputs)             # (batch, C)\n",
        "        y = tf.expand_dims(y, axis=-1)        # (batch, C, 1)\n",
        "        y = self.conv1d(y)                     # (batch, C, 1)\n",
        "        y = self.sigmoid(y)\n",
        "        y = tf.reshape(y, [-1, 1, 1, inputs.shape[-1]])  # (batch, 1, 1, C)\n",
        "        return inputs * y                      # broadcast across H, W\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model with ECA\n",
        "# -----------------------------\n",
        "def create_resnet_eca(image_shape=(224,224,3), num_classes=n_classes):\n",
        "    base_model = tf.keras.applications.ResNet50(input_shape=image_shape,\n",
        "                                                   include_top=False,\n",
        "                                                   weights=\"imagenet\")\n",
        "    base_model.trainable = True\n",
        "    for layer in base_model.layers[0:291]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = tf.keras.Input(shape=image_shape)\n",
        "    x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
        "    x = base_model(x, training=False)          # (None, H, W, 2048)\n",
        "\n",
        "    # Insert fixed ECA\n",
        "    x = ECABlock(channels=2048, name=\"eca_block\")(x)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "inVuDFZ9TWA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Build and Compile Model"
      ],
      "metadata": {
        "id": "hBDhW1A4nqHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Build and Compile\n",
        "# -----------------------------\n",
        "base_learning_rate = 0.0001\n",
        "n_classes = 3  # change this according to your dataset\n",
        "\n",
        "model = create_resnet_eca(image_shape=(224,224,3), num_classes=n_classes)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Nadam(learning_rate=base_learning_rate),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "wAwYzw5dWrLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Load / Set Model Weights\n",
        "Load pre-trained weights or initialize custom weights for the CNN + CBAM model.\n"
      ],
      "metadata": {
        "id": "k61jL8dRnqHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you already have Dataset as a list of samples per class\n",
        "# or you can compute from train_dataset using:\n",
        "# class_counts = [sum(1 for _, label in train_dataset.unbatch().as_numpy_iterator() if label==i) for i in range(n_classes)]\n",
        "\n",
        "total = sum(Dataset)        # total number of images\n",
        "class_weight = {}\n",
        "\n",
        "for i, count in enumerate(Dataset):\n",
        "    class_weight[i] = (1 / count) * (total / len(Dataset))\n",
        "\n",
        "# Print class weights\n",
        "for i, w in class_weight.items():\n",
        "    print(f\"Weight for class {i}: {w:.2f}\")\n"
      ],
      "metadata": {
        "id": "93csF4wNnqHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Model Checkpoint & Save\n",
        "Save the best model during training using **checkpoints**, and optionally save the final trained model.\n"
      ],
      "metadata": {
        "id": "Z0SD6J9-nqHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "## set the path name as *dataset/Lr/optimizer_name/model_name*\n",
        "model_filepath=\"/content/drive/MyDrive/sample_dataset/resnet50_eca-{epoch:02d}-{val_accuracy:.4f}.keras\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath = model_filepath ,\n",
        "    monitor ='val_accuracy',\n",
        "    mode = 'max' ,\n",
        "    save_best_only =True ,\n",
        "    verbose = 1\n",
        ")\n"
      ],
      "metadata": {
        "id": "5u1QNwNvnqHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Train the Model\n",
        "Train the CNN + CBAM model using the training dataset, validate on the validation dataset, and store the training history.\n"
      ],
      "metadata": {
        "id": "2HU4EbEBnqHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## change hyperparameter such as epoches\n",
        "history = model.fit(train_dataset , verbose=2 , epochs=5 , class_weight=class_weight ,                         validation_data=valid_dataset ,\n",
        "                    callbacks =[checkpoint])"
      ],
      "metadata": {
        "id": "25NYKRDEliJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Test the Model\n",
        "Evaluate the trained model on the test dataset."
      ],
      "metadata": {
        "id": "zH73mRRfnTlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset , verbose = 1)"
      ],
      "metadata": {
        "id": "hoNRTfI3nTlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.3: Results & Visualizations\n"
      ],
      "metadata": {
        "id": "Nvpe7nHZnqHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Actual vs Predicted Classes\n",
        "\n",
        "*  Visualize the modelâ€™s predictions compared to true labels on the test dataset.\n",
        "*   Collect **one example per class** from `test_dataset`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WVpNWLhioCq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = {}\n",
        "for images, labels in test_dataset.unbatch().take(1000):  # take enough to find all classes\n",
        "    class_idx = labels.numpy()\n",
        "    if class_idx not in examples:\n",
        "        examples[class_idx] = images\n",
        "    if len(examples) == n_classes:\n",
        "        break\n",
        "\n",
        "# Plotting\n",
        "cols = 2  # number of columns\n",
        "rows = math.ceil(n_classes / cols)\n",
        "plt.figure(figsize=(cols * 5, rows * 5))\n",
        "\n",
        "for i, class_idx in enumerate(sorted(examples.keys())):\n",
        "    img = examples[class_idx].numpy().astype(\"uint8\")\n",
        "    img_exp = tf.expand_dims(img, 0)  # expand batch dim\n",
        "    predict = model.predict(img_exp)\n",
        "    predicted = class_names[np.argmax(predict)]\n",
        "    actual = class_names[class_idx]\n",
        "\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    color = 'blue' if predicted == actual else 'red'\n",
        "    plt.title(f\"Pred: {predicted}\\nActual: {actual}\", fontsize=12, fontweight='bold', color=color)\n",
        "    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9,\n",
        "                        top=0.9, wspace=0.4, hspace=0.4)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hrIS09IaoCq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Actual vs Predicted Images\n",
        "\n",
        "*   Display sample images from the test set with their **true labels and model predictions** for qualitative evaluation.\n",
        "*   Show **three images per class** for qualitative evaluation.\n"
      ],
      "metadata": {
        "id": "E9nz2NY1oCrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load multiple random images per class\n",
        "def load_images_per_class(folder, num_images_per_class=3):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "\n",
        "    for class_idx, subfolder in enumerate(sorted(os.listdir(folder))):\n",
        "        subfolder_path = os.path.join(folder, subfolder)\n",
        "        if os.path.isdir(subfolder_path):\n",
        "            class_names.append(subfolder)\n",
        "            image_files = os.listdir(subfolder_path)\n",
        "            selected_files = random.sample(image_files, min(num_images_per_class, len(image_files)))\n",
        "            for image_file in selected_files:\n",
        "                img_path = os.path.join(subfolder_path, image_file)\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img = img.resize((64, 64))\n",
        "                images.append(np.array(img)/255.0)\n",
        "                labels.append(class_idx)\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "# Path to your test folder\n",
        "test_folder = '/content/drive/MyDrive/sample_dataset/test'\n",
        "\n",
        "# Load images: change num_images_per_class as needed\n",
        "num_images_per_class = 3\n",
        "images, labels, class_names = load_images_per_class(test_folder, num_images_per_class=num_images_per_class)\n",
        "\n",
        "# Example predicted labels (replace with your model predictions)\n",
        "predicted_labels = labels.copy()  # For demo, assume correct predictions\n",
        "\n",
        "# Automatically calculate subplot grid\n",
        "total_images = len(images)\n",
        "cols = 2  # Original + Predicted\n",
        "rows = total_images  # Each image gets a row\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*4))\n",
        "\n",
        "if rows == 1:  # Special case if only 1 image\n",
        "    axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "for i in range(total_images):\n",
        "    img = images[i]\n",
        "    true_label = class_names[labels[i]]\n",
        "    predicted_label = class_names[predicted_labels[i]]\n",
        "\n",
        "    # Original\n",
        "    axes[i, 0].imshow(img)\n",
        "    axes[i, 0].set_title(f'True: {true_label}', fontsize=12, fontweight='bold')\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    # Predicted\n",
        "    axes[i, 1].imshow(img)\n",
        "    color = 'blue' if true_label == predicted_label else 'red'\n",
        "    axes[i, 1].set_title(f'Predicted: {predicted_label}', fontsize=12, fontweight='bold', color=color)\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oaadYn-XoCrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ“ˆ Training Accuracy & Loss\n",
        "Visualize the model's **training and validation accuracy and loss** over epochs to assess learning and overfitting."
      ],
      "metadata": {
        "id": "c6-vit70oCq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(17, 12))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy', fontweight='bold')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,3.0])\n",
        "plt.title('Training and Validation Loss', fontweight='bold')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2CfNCt4oCq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Label Binarization & Classification Metrics\n",
        "Binarize class labels and evaluate model performance using metrics like **accuracy, precision, recall, and F1-score**.\n"
      ],
      "metadata": {
        "id": "sikDz_BLoCq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Initialize empty lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the validation dataset and make predictions\n",
        "for images, labels in test_dataset:\n",
        "    predictions = model.predict(images)\n",
        "    predicted_labels.extend(np.argmax(predictions, axis=1))\n",
        "    true_labels.extend(labels.numpy())\n",
        "\n",
        "# Binarize the true and predicted labels\n",
        "true_labels_bin = label_binarize(true_labels, classes=np.unique(true_labels))\n",
        "predicted_labels_bin = label_binarize(predicted_labels, classes=np.unique(predicted_labels))"
      ],
      "metadata": {
        "id": "d1y8GZc-oCq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Classification Report\n",
        "Provides precision, recall, F1-score, and support for each class to summarize model performance."
      ],
      "metadata": {
        "id": "ci5breZgnYw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=class_names)\n",
        "\n",
        "# Print the classification report\n",
        "print(report)"
      ],
      "metadata": {
        "id": "mGJIz8EyoCq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ“ˆ ROC curve\n",
        "*   Plot the **ROC curve** to evaluate model performance.\n",
        "*  **One-vs-Rest** for multiclass and\n",
        " **One-vs-One** for binary classification."
      ],
      "metadata": {
        "id": "o88-JPVjoCq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Get true labels and predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_dataset:     # or valid_dataset if you want\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(preds)\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "print(\"Shape of predictions:\", y_pred.shape)\n"
      ],
      "metadata": {
        "id": "hzY8IiwooCq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarize labels for multiclass ROC\n",
        "y_true_bin = label_binarize(y_true, classes=list(range(n_classes)))"
      ],
      "metadata": {
        "id": "yZYvG7d9oCq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n"
      ],
      "metadata": {
        "id": "2nHKeqzqoCq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Automatically generate colors\n",
        "colors = cm.get_cmap('tab20', n_classes)  # 'tab20' or 'tab10', n_classes colors\n",
        "\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], color=colors(i), lw=2,\n",
        "             label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve', fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z3AlvlCeoCq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Confusion Matrix\n",
        "Visualize the **confusion matrix** to assess class-wise prediction performance."
      ],
      "metadata": {
        "id": "m-dJCfkpoCrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix as a heatmap\n",
        "plt.figure(figsize=(5, 3))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix', fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8t6spBzToCrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Cohen's Kappa\n",
        "Measure agreement between predicted and true labels beyond chance."
      ],
      "metadata": {
        "id": "1CLpjA_moCrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "kappa = cohen_kappa_score(true_labels, predicted_labels)\n",
        "print(f'Cohen\\'s Kappa: {kappa:.4f}')"
      ],
      "metadata": {
        "id": "At7ggZiioCrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Matthews Correlation Coefficient (MCC)\n",
        "Assess overall classification quality considering all confusion matrix terms."
      ],
      "metadata": {
        "id": "jJFO8Md-oCrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Assuming true_labels and predicted_labels are multiclass labels\n",
        "mcc_values = [matthews_corrcoef(true_labels == i, predicted_labels == i) for i in np.unique(true_labels)]\n",
        "\n",
        "average_mcc = np.mean(mcc_values)\n",
        "print(f'Average Matthews Correlation Coefficient for Multiclass: {average_mcc:.4f}')"
      ],
      "metadata": {
        "id": "dO2GU6fgoCrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Right vs Wrong Classifier\n",
        "Visualize and analyze **correctly and incorrectly classified samples** to understand model performance."
      ],
      "metadata": {
        "id": "bJZzIEo5nxbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total right and wrong predictions\n",
        "total_right = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "total_wrong = sum(1 for true, pred in zip(true_labels, predicted_labels) if true != pred)\n",
        "total_samples = len(true_labels)\n",
        "\n",
        "# Calculate wrong prediction percentage\n",
        "wrong_prediction_percentage = (total_wrong / total_samples) * 100\n",
        "\n",
        "print(\"Total Right Predictions:\", total_right)\n",
        "print(\"Total Wrong Predictions:\", total_wrong)\n",
        "print(\"Wrong Prediction Percentage: {:.2f}%\".format(wrong_prediction_percentage))\n"
      ],
      "metadata": {
        "id": "_6WhR4nfnxbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for plotting\n",
        "categories = ['Right Predictions', 'Wrong Predictions']\n",
        "values = [total_right, total_wrong]\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(categories, values, color=['green', 'red'])\n",
        "plt.ylabel('Count')\n",
        "plt.title('Right and Wrong Predictions', fontweight='bold')\n",
        "\n",
        "# Show counts on top of the bars\n",
        "for i, v in enumerate(values):\n",
        "    plt.text(i, v + 0.1, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RfGnbCwgnxbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Sensitivity & Specificity\n",
        "Evaluate each class's **sensitivity (recall)** and **specificity** based on true positives and true negatives\n"
      ],
      "metadata": {
        "id": "JqLRXuskoCrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "# If binary classification\n",
        "if n_classes == 2:\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    print(\"Binary Classification:\")\n",
        "    print(f\"Sensitivity (Recall): {sensitivity:.3f}\")\n",
        "    print(f\"Specificity: {specificity:.3f}\")\n",
        "\n",
        "# If multiclass classification\n",
        "else:\n",
        "    # Sensitivity = TP / (TP + FN)\n",
        "    sensitivity = np.diag(cm) / np.sum(cm, axis=1)\n",
        "    # Specificity = TN / (TN + FP)\n",
        "    specificity = []\n",
        "    for i in range(n_classes):\n",
        "        # For each class, treat it as \"positive\" vs \"rest\"\n",
        "        tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
        "        fp = np.sum(np.delete(cm, i, axis=0)[:, i])\n",
        "        specificity.append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
        "    specificity = np.array(specificity)\n",
        "\n",
        "    print(\"Multiclass Classification:\")\n",
        "    for i in range(n_classes):\n",
        "        print(f\"Class {i}: Sensitivity={sensitivity[i]:.3f}, Specificity={specificity[i]:.3f}\")\n",
        "\n",
        "    print(f\"\\nAverage Sensitivity: {np.mean(sensitivity):.3f}\")\n",
        "    print(f\"Average Specificity: {np.mean(specificity):.3f}\")\n"
      ],
      "metadata": {
        "id": "lTD8NkhBoCrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot Sensitivity and Specificity ---\n",
        "if n_classes == 2:\n",
        "    metrics = ['Sensitivity', 'Specificity']\n",
        "    values = [sensitivity, specificity]\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.bar(metrics, values, color=['#1f77b4', '#ff7f0e'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(\"Binary Classification Metrics\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    x = np.arange(n_classes)\n",
        "    width = 0.35  # Bar width\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(x - width/2, sensitivity, width, label='Sensitivity', color='#1f77b4')\n",
        "    plt.bar(x + width/2, specificity, width, label='Specificity', color='#ff7f0e')\n",
        "\n",
        "    plt.xticks(x, [f'Class {i}' for i in range(n_classes)])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(\"Per-Class Sensitivity and Specificity\", fontsize=12, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sR5aeppkoCrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "# Sensitivity (macro average): TP / (TP + FN)\n",
        "sensitivity_per_class = np.diag(cm) / np.sum(cm, axis=1)\n",
        "sensitivity = np.nanmean(sensitivity_per_class)\n",
        "\n",
        "# Specificity (macro average): TN / (TN + FP)\n",
        "specificity_list = []\n",
        "for i in range(n_classes):\n",
        "    tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))\n",
        "    fp = np.sum(np.delete(cm, i, axis=0)[:, i])\n",
        "    specificity_list.append(tn / (tn + fp) if (tn + fp) > 0 else np.nan)\n",
        "specificity = np.nanmean(specificity_list)\n",
        "\n",
        "print(f\"Overall Sensitivity (Recall): {sensitivity:.3f}\")\n",
        "print(f\"Overall Specificity: {specificity:.3f}\")\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.bar(['Sensitivity', 'Specificity'], [sensitivity, specificity], color=['skyblue', 'salmon'])\n",
        "plt.title('Overall Sensitivity and Specificity', fontsize=12, fontweight='bold')\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "deYzr4mLoCrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Jaccard Index & Dice Score\n",
        "Measure overlap between predicted and true classes using **Jaccard Index (IoU)** and **Dice Score**.\n"
      ],
      "metadata": {
        "id": "8yFm6N9boCrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "n_classes = cm.shape[0]\n",
        "\n",
        "jaccard_per_class = []\n",
        "dice_per_class = []\n",
        "\n",
        "for i in range(n_classes):\n",
        "    tp = cm[i, i]\n",
        "    fp = np.sum(cm[:, i]) - tp\n",
        "    fn = np.sum(cm[i, :]) - tp\n",
        "\n",
        "    # Jaccard Index\n",
        "    jaccard = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else np.nan\n",
        "    jaccard_per_class.append(jaccard)\n",
        "\n",
        "    # Dice Score = 2*TP / (2*TP + FP + FN)\n",
        "    dice = 2*tp / (2*tp + fp + fn) if (2*tp + fp + fn) > 0 else np.nan\n",
        "    dice_per_class.append(dice)\n",
        "\n",
        "# Macro-average (overall)\n",
        "jaccard_index = np.nanmean(jaccard_per_class)\n",
        "dice_score = np.nanmean(dice_per_class)\n",
        "\n",
        "print(f\"Overall Jaccard Index: {jaccard_index:.3f}\")\n",
        "print(f\"Overall Dice Score: {dice_score:.3f}\")\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(['Jaccard Index', 'Dice Score'], [jaccard_index, dice_score], color=['lightgreen', 'skyblue'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('Overall Jaccard Index and Dice Score', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rczhy2zQoCrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Model Evaluation Metrics\n",
        "Summarize the model's performance using multiple metrics: **Accuracy**, **Precision**, **Recall (Sensitivity)**, **F1 Score**, **Negative Predictive Value (NPV)**, **AUC-ROC**"
      ],
      "metadata": {
        "id": "AlCFpO_HoCrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Number of classes\n",
        "n_classes = len(np.unique(true_labels))\n",
        "\n",
        "# Accuracy, F1, Precision, Recall (macro average for multiclass)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "\n",
        "# For binary: compute NPV and AUC\n",
        "if n_classes == 2:\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "    auc = roc_auc_score(true_labels, predicted_labels)\n",
        "else:\n",
        "    npv = np.nan  # Not defined for multiclass\n",
        "    auc = np.nan  # Not defined for multiclass\n",
        "\n",
        "# Store metrics in a dictionary\n",
        "metrics = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1,\n",
        "    'NPV': npv,\n",
        "    'AUC-ROC': auc\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(metrics.keys(), metrics.values(), color=['blue', 'orange', 'green', 'red', 'purple', 'cyan'])\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Evaluation Metrics', fontsize=12, fontweight='bold')\n",
        "plt.ylim(0, 1)\n",
        "plt.axhline(y=0.5, color='grey', linestyle='--')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aUt2PuSnoCrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print metric values\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "id": "faPPiDjDoCrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¥ Grad-CAM Visualization\n",
        "Generate **Grad-CAM heatmaps** to visualize the regions of input images that the **CNN + CBAM** model focuses on for its predictions.\n"
      ],
      "metadata": {
        "id": "FNUnLqcXoCrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Grad-CAM Function\n",
        "# -----------------------------\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# Display Heatmap Function\n",
        "# -----------------------------\n",
        "def display_gradcam(img_path, heatmap, alpha=0.4):\n",
        "    img = image.load_img(img_path, target_size=(224,224))\n",
        "    img = image.img_to_array(img)\n",
        "\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = np.expand_dims(heatmap, axis=2)\n",
        "    heatmap = np.repeat(heatmap, 3, axis=2)\n",
        "    heatmap = tf.image.resize(heatmap, (img.shape[0], img.shape[1])).numpy()\n",
        "\n",
        "    superimposed_img = heatmap * alpha + img\n",
        "    superimposed_img = np.uint8(superimposed_img)\n",
        "\n",
        "    plt.imshow(superimposed_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Loop Through Classes\n",
        "# -----------------------------\n",
        "def gradcam_per_class(model, data_dir, last_conv_layer_name=\"cbam\"):\n",
        "\n",
        "    class_folders = sorted(os.listdir(data_dir))\n",
        "\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        # Take first image in folder\n",
        "        img_name = os.listdir(class_path)[0]\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img = image.load_img(img_path, target_size=(224,224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = tf.keras.applications.inception_v3.preprocess_input(img_array)\n",
        "\n",
        "        # Generate Grad-CAM\n",
        "        heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "\n",
        "        # Plot\n",
        "        print(f\"Grad-CAM for class: {class_name}, image: {img_name}\")\n",
        "        display_gradcam(img_path, heatmap)\n",
        "\n",
        "# -----------------------------\n",
        "# Usage\n",
        "# -----------------------------\n",
        "data_dir = \"/content/drive/MyDrive/sample_dataset/test\"  # folder containing subfolders of each class\n",
        "gradcam_per_class(model, data_dir)\n"
      ],
      "metadata": {
        "id": "hjP7klekoCrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# -----------------------------\n",
        "# Grad-CAM Function\n",
        "# -----------------------------\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs],\n",
        "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(predictions[0])\n",
        "        class_channel = predictions[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(class_channel, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# -----------------------------\n",
        "# Overlay Heatmap on Image\n",
        "# -----------------------------\n",
        "def overlay_heatmap_on_image(img_path, heatmap, alpha=0.4):\n",
        "    # Load original image\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img = img.resize((224,224))\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Convert heatmap to RGB using jet colormap\n",
        "    # Use matplotlib.colormaps.get_cmap to access the colormap\n",
        "    jet_colormap = matplotlib.colormaps.get_cmap('jet')\n",
        "    heatmap_colored = jet_colormap(heatmap)[..., :3]  # RGB channels only\n",
        "    heatmap_colored = np.uint8(heatmap_colored * 255)\n",
        "    heatmap_image = Image.fromarray(heatmap_colored)\n",
        "\n",
        "    # Resize heatmap to match original image using updated Pillow method\n",
        "    heatmap_image = heatmap_image.resize(img_array.shape[:2][::-1], Image.Resampling.LANCZOS)\n",
        "    heatmap_resized = np.array(heatmap_image)\n",
        "\n",
        "    # Superimpose heatmap\n",
        "    superimposed_img = np.uint8(alpha * heatmap_resized + (1 - alpha) * img_array)\n",
        "\n",
        "    return Image.fromarray(superimposed_img)\n",
        "\n",
        "# -----------------------------\n",
        "# Grad-CAM for Each Class\n",
        "# -----------------------------\n",
        "def gradcam_per_class(model, data_dir):\n",
        "    # Automatically get CBAM layer name\n",
        "    cbam_layer_name = [layer.name for layer in model.layers if \"cbam\" in layer.name][0]\n",
        "    print(\"Using CBAM layer:\", cbam_layer_name)\n",
        "\n",
        "    class_folders = sorted(os.listdir(data_dir))\n",
        "    for class_name in class_folders:\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        # Pick first image of class\n",
        "        img_name = os.listdir(class_path)[0]\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img = image.load_img(img_path, target_size=(224,224))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "        img_array = tf.keras.applications.inception_v3.preprocess_input(img_array)\n",
        "\n",
        "        # Compute Grad-CAM heatmap\n",
        "        heatmap = make_gradcam_heatmap(img_array, model, cbam_layer_name)\n",
        "\n",
        "        # Overlay heatmap\n",
        "        superimposed_img = overlay_heatmap_on_image(img_path, heatmap)\n",
        "\n",
        "        # Display\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.title(f\"Class: {class_name}, Image: {img_name}\")\n",
        "        plt.imshow(superimposed_img)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Usage\n",
        "# -----------------------------\n",
        "data_dir = \"/content/drive/MyDrive/sample_dataset/test\"  # folder with subfolders for each class\n",
        "gradcam_per_class(model, data_dir)"
      ],
      "metadata": {
        "id": "LpujK_0toCrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ PR-AUC (Precision-Recall AUC)\n",
        "Evaluate model performance using the **area under the Precision-Recall curve**, especially useful for imbalanced datasets."
      ],
      "metadata": {
        "id": "dDOvOPfToCrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Get true labels and predicted probabilities\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred_probs = model.predict(test_dataset)\n",
        "\n",
        "# Handle binary or multiclass\n",
        "if y_pred_probs.shape[1] == 2:  # binary\n",
        "    precision, recall, _ = precision_recall_curve(y_true, y_pred_probs[:, 1])\n",
        "    pr_auc = auc(recall, precision)\n",
        "else:  # multiclass\n",
        "    pr_auc = {}\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "    y_true_bin = label_binarize(y_true, classes=range(y_pred_probs.shape[1]))\n",
        "    for i in range(y_pred_probs.shape[1]):\n",
        "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_probs[:, i])\n",
        "        pr_auc[i] = auc(recall, precision)\n",
        "\n",
        "print(\"PR-AUC:\", pr_auc)\n"
      ],
      "metadata": {
        "id": "SXCEPCBNoCrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Log Loss / Cross-Entropy Loss\n",
        "Evaluate prediction confidence using **log loss** (cross-entropy) between true and predicted probabilities."
      ],
      "metadata": {
        "id": "wMvVRyONoCrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred_probs = model.predict(test_dataset)\n",
        "\n",
        "loss = log_loss(y_true, y_pred_probs)\n",
        "print(\"Log Loss / Cross-Entropy Loss:\", loss)\n"
      ],
      "metadata": {
        "id": "lQ7DFrGdoCrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ Top-k Accuracy\n",
        "Measure if the **true label** is among the model's **top k predicted classes** in multiclass classification."
      ],
      "metadata": {
        "id": "j9ZIwksXoCrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-k accuracy automatically for multiclass\n",
        "k = 3  # you can change k\n",
        "top_k_acc = tf.keras.metrics.TopKCategoricalAccuracy(k=k)\n",
        "\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred_probs = model.predict(test_dataset)\n",
        "\n",
        "# Convert binary labels to categorical if needed\n",
        "if y_pred_probs.shape[1] == 2 and len(np.unique(y_true)) == 2:\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    y_true_cat = to_categorical(y_true, num_classes=2)\n",
        "else:\n",
        "    y_true_cat = tf.keras.utils.to_categorical(y_true, num_classes=y_pred_probs.shape[1])\n",
        "\n",
        "top_k_acc.update_state(y_true_cat, y_pred_probs)\n",
        "print(f\"Top-{k} Accuracy:\", top_k_acc.result().numpy())"
      ],
      "metadata": {
        "id": "uU_W6uRaoCrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ðŸ”¹ G-Mean (Geometric Mean of Sensitivity & Specificity)\n",
        "Compute the **G-Mean** to evaluate balanced classification performance."
      ],
      "metadata": {
        "id": "_TPkKFiyoCrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.concatenate([y.numpy() for x, y in test_dataset], axis=0)\n",
        "y_pred = np.argmax(model.predict(test_dataset), axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "if cm.shape[0] == 2:  # binary\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    sensitivity = tp / (tp + fn)\n",
        "    specificity = tn / (tn + fp)\n",
        "    g_mean = math.sqrt(sensitivity * specificity)\n",
        "else:  # multiclass: compute G-mean per class and average\n",
        "    sensitivity_list = []\n",
        "    specificity_list = []\n",
        "    for i in range(cm.shape[0]):\n",
        "        tp = cm[i, i]\n",
        "        fn = cm[i, :].sum() - tp\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        tn = cm.sum() - (tp + fn + fp)\n",
        "        sensitivity_list.append(tp / (tp + fn) if (tp+fn)>0 else 0)\n",
        "        specificity_list.append(tn / (tn + fp) if (tn+fp)>0 else 0)\n",
        "    g_mean = np.mean(np.sqrt(np.array(sensitivity_list) * np.array(specificity_list)))\n",
        "\n",
        "print(\"G-Mean:\", g_mean)"
      ],
      "metadata": {
        "id": "IR0BWi82oCrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}